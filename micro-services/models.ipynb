{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing URL: https://x.com/unfilteredBren/status/1937329720091373575\n",
      "==================================================\n",
      "Trying 2025 Playwright XHR method...\n",
      "Trying alternative API endpoints...\n",
      "Final Result: {\n",
      "  \"text\": \"Everytime when something related to the USA is hit during war it's ceasefire.\\nInd Vs Pak( Nur khan Base)\\nIsrael vs Iran (US Air Base in Qatar)\\n\\nSo its Trump'S Surrender in the war where US indirectly involved \\n#IranIsraelConflict \\n#IndiaPakistanWar \\nINC walo ab kya karoge https://t.co/kN5SmiYBCl\",\n",
      "  \"method\": \"api_endpoint\",\n",
      "  \"api_url\": \"https://cdn.syndication.twimg.com/tweet-result?id=1937329720091373575&lang=en&token=1\"\n",
      "}\n",
      "\n",
      "==================================================\n",
      "Installation Requirements:\n",
      "pip install playwright jmespath\n",
      "playwright install\n",
      "\n",
      "For production use, consider:\n",
      "1. Official Twitter API v2\n",
      "2. Paid scraping services (ScrapFly, etc.)\n",
      "3. twscrape library with account setup\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import json\n",
    "from urllib.parse import urlparse\n",
    "import time\n",
    "\n",
    "def get_twitter_post_content_2025(post_url):\n",
    "    \"\"\"\n",
    "    Updated method for 2025 - captures background XHR requests that contain tweet data\n",
    "    Uses Playwright to intercept TweetResultByRestId requests\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from playwright.sync_api import sync_playwright\n",
    "        import jmespath\n",
    "        \n",
    "        _xhr_calls = []\n",
    "        \n",
    "        def intercept_response(response):\n",
    "            \"\"\"Capture all background requests and save them\"\"\"\n",
    "            if response.request.resource_type == \"xhr\":\n",
    "                _xhr_calls.append(response)\n",
    "            return response\n",
    "        \n",
    "        with sync_playwright() as pw:\n",
    "            browser = pw.chromium.launch(headless=True)\n",
    "            context = browser.new_context(\n",
    "                viewport={\"width\": 1920, \"height\": 1080},\n",
    "                user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "            )\n",
    "            page = context.new_page()\n",
    "            \n",
    "            # Enable background request intercepting\n",
    "            page.on(\"response\", intercept_response)\n",
    "            \n",
    "            # Navigate to the tweet URL\n",
    "            page.goto(post_url)\n",
    "            \n",
    "            # Wait for tweet to load\n",
    "            try:\n",
    "                page.wait_for_selector(\"[data-testid='tweet']\", timeout=10000)\n",
    "            except:\n",
    "                page.wait_for_timeout(5000)  # Wait a bit anyway\n",
    "            \n",
    "            # Find tweet background requests\n",
    "            tweet_calls = [f for f in _xhr_calls if \"TweetResultByRestId\" in f.url]\n",
    "            \n",
    "            browser.close()\n",
    "            \n",
    "            if not tweet_calls:\n",
    "                return {\"error\": \"No tweet data found in background requests\"}\n",
    "            \n",
    "            # Extract data from the first valid response\n",
    "            for xhr in tweet_calls:\n",
    "                try:\n",
    "                    data = xhr.json()\n",
    "                    tweet_result = data.get('data', {}).get('tweetResult', {}).get('result', {})\n",
    "                    \n",
    "                    if tweet_result:\n",
    "                        # Parse the complex Twitter data structure\n",
    "                        parsed_tweet = parse_tweet_data(tweet_result)\n",
    "                        return parsed_tweet\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            return {\"error\": \"Could not parse tweet data from background requests\"}\n",
    "                    \n",
    "    except ImportError:\n",
    "        return {\"error\": \"Playwright not installed. Run: pip install playwright jmespath && playwright install\"}\n",
    "    except Exception as e:\n",
    "        return {\"error\": f\"Playwright scraping failed: {str(e)}\"}\n",
    "\n",
    "def parse_tweet_data(tweet_data):\n",
    "    \"\"\"Parse the complex Twitter JSON response to extract useful information\"\"\"\n",
    "    try:\n",
    "        import jmespath\n",
    "        \n",
    "        # Use jmespath to extract key fields from the complex nested structure\n",
    "        parsed = jmespath.search(\"\"\"\n",
    "        {\n",
    "            text: legacy.full_text,\n",
    "            created_at: legacy.created_at,\n",
    "            retweet_count: legacy.retweet_count,\n",
    "            favorite_count: legacy.favorite_count,\n",
    "            reply_count: legacy.reply_count,\n",
    "            quote_count: legacy.quote_count,\n",
    "            bookmark_count: legacy.bookmark_count,\n",
    "            view_count: views.count,\n",
    "            language: legacy.lang,\n",
    "            tweet_id: legacy.id_str,\n",
    "            conversation_id: legacy.conversation_id_str,\n",
    "            hashtags: legacy.entities.hashtags[].text,\n",
    "            urls: legacy.entities.urls[].expanded_url,\n",
    "            user_mentions: legacy.entities.user_mentions[].screen_name,\n",
    "            media: legacy.entities.media[].media_url_https,\n",
    "            is_retweet: legacy.retweeted,\n",
    "            is_quote: legacy.is_quote_status,\n",
    "            source: source\n",
    "        }\n",
    "        \"\"\", tweet_data)\n",
    "        \n",
    "        # Extract user information\n",
    "        user_data = jmespath.search(\"core.user_results.result\", tweet_data)\n",
    "        if user_data:\n",
    "            user_info = jmespath.search(\"\"\"\n",
    "            {\n",
    "                name: legacy.name,\n",
    "                screen_name: legacy.screen_name,\n",
    "                description: legacy.description,\n",
    "                followers_count: legacy.followers_count,\n",
    "                friends_count: legacy.friends_count,\n",
    "                verified: legacy.verified,\n",
    "                profile_image: legacy.profile_image_url_https\n",
    "            }\n",
    "            \"\"\", user_data)\n",
    "            parsed['user'] = user_info\n",
    "        \n",
    "        parsed['method'] = 'playwright_xhr_2025'\n",
    "        return parsed\n",
    "        \n",
    "    except ImportError:\n",
    "        # Fallback parsing without jmespath\n",
    "        result = {}\n",
    "        legacy = tweet_data.get('legacy', {})\n",
    "        \n",
    "        result['text'] = legacy.get('full_text', '')\n",
    "        result['created_at'] = legacy.get('created_at', '')\n",
    "        result['retweet_count'] = legacy.get('retweet_count', 0)\n",
    "        result['favorite_count'] = legacy.get('favorite_count', 0)\n",
    "        result['reply_count'] = legacy.get('reply_count', 0)\n",
    "        result['tweet_id'] = legacy.get('id_str', '')\n",
    "        result['method'] = 'playwright_xhr_2025_fallback'\n",
    "        \n",
    "        # Extract user info\n",
    "        user_result = tweet_data.get('core', {}).get('user_results', {}).get('result', {})\n",
    "        if user_result:\n",
    "            user_legacy = user_result.get('legacy', {})\n",
    "            result['user'] = {\n",
    "                'name': user_legacy.get('name', ''),\n",
    "                'screen_name': user_legacy.get('screen_name', ''),\n",
    "                'followers_count': user_legacy.get('followers_count', 0)\n",
    "            }\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    except Exception as e:\n",
    "        return {\"error\": f\"Failed to parse tweet data: {str(e)}\"}\n",
    "\n",
    "def get_twitter_content_twscrape(post_url):\n",
    "    \"\"\"\n",
    "    Alternative using twscrape library - requires setup but very reliable\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # This would require: pip install twscrape\n",
    "        # And account setup, so returning info instead\n",
    "        return {\n",
    "            \"error\": \"twscrape method requires setup\",\n",
    "            \"info\": \"For production use, consider twscrape library: pip install twscrape\",\n",
    "            \"setup_required\": \"Account authentication needed\"\n",
    "        }\n",
    "    except:\n",
    "        return {\"error\": \"twscrape not available\"}\n",
    "\n",
    "def get_twitter_content_api_alternative(post_url):\n",
    "    \"\"\"\n",
    "    Try alternative API endpoints that might still work\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extract tweet ID\n",
    "        tweet_id_match = re.search(r'/status/(\\d+)', post_url)\n",
    "        if not tweet_id_match:\n",
    "            return {\"error\": \"Could not extract tweet ID\"}\n",
    "        \n",
    "        tweet_id = tweet_id_match.group(1)\n",
    "        \n",
    "        # Try different API endpoints\n",
    "        api_urls = [\n",
    "            f\"https://api.twitter.com/1.1/statuses/show.json?id={tweet_id}\",\n",
    "            f\"https://api.twitter.com/2/tweets/{tweet_id}?expansions=author_id&tweet.fields=created_at,public_metrics,text\",\n",
    "            f\"https://cdn.syndication.twimg.com/tweet-result?id={tweet_id}&lang=en&token=1\"\n",
    "        ]\n",
    "        \n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n",
    "            'Accept': 'application/json',\n",
    "            'Referer': 'https://x.com/',\n",
    "        }\n",
    "        \n",
    "        for api_url in api_urls:\n",
    "            try:\n",
    "                response = requests.get(api_url, headers=headers, timeout=10)\n",
    "                if response.status_code == 200:\n",
    "                    data = response.json()\n",
    "                    if 'text' in data or 'full_text' in data:\n",
    "                        return {\n",
    "                            'text': data.get('text', data.get('full_text', '')),\n",
    "                            'method': 'api_endpoint',\n",
    "                            'api_url': api_url\n",
    "                        }\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        return {\"error\": \"All API endpoints failed\"}\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\"error\": f\"API method failed: {str(e)}\"}\n",
    "\n",
    "# Updated main function with the latest 2025 method\n",
    "def get_twitter_post_content_robust_2025(post_url):\n",
    "    \"\"\"\n",
    "    Most current method for 2025 - tries the latest working approaches\n",
    "    \"\"\"\n",
    "    if not is_valid_twitter_url(post_url):\n",
    "        return {\"error\": \"Invalid X/Twitter URL\"}\n",
    "    \n",
    "    print(\"Trying 2025 Playwright XHR method...\")\n",
    "    result = get_twitter_post_content_2025(post_url)\n",
    "    if 'error' not in result and result.get('text'):\n",
    "        return result\n",
    "    \n",
    "    print(\"Trying alternative API endpoints...\")\n",
    "    result = get_twitter_content_api_alternative(post_url)\n",
    "    if 'error' not in result and result.get('text'):\n",
    "        return result\n",
    "    \n",
    "    print(\"Trying Nitter instances...\")\n",
    "    result = get_twitter_content_via_nitter(post_url)\n",
    "    if 'error' not in result:\n",
    "        return result\n",
    "    \n",
    "    return {\"error\": \"All 2025 methods failed\", \"suggestion\": \"Consider using official Twitter API v2 or paid scraping services\"}\n",
    "\n",
    "def is_valid_twitter_url(url):\n",
    "    \"\"\"Check if the URL is a valid X/Twitter post URL\"\"\"\n",
    "    try:\n",
    "        parsed = urlparse(url)\n",
    "        return (parsed.netloc in ['twitter.com', 'www.twitter.com', 'x.com', 'www.x.com'] and \n",
    "                '/status/' in parsed.path)\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def extract_from_meta_tags(soup):\n",
    "    \"\"\"Extract content from Open Graph and Twitter meta tags\"\"\"\n",
    "    content = {}\n",
    "    \n",
    "    # Try Open Graph tags\n",
    "    og_title = soup.find('meta', property='og:title')\n",
    "    og_description = soup.find('meta', property='og:description')\n",
    "    \n",
    "    # Try Twitter meta tags\n",
    "    twitter_title = soup.find('meta', attrs={'name': 'twitter:title'})\n",
    "    twitter_description = soup.find('meta', attrs={'name': 'twitter:description'})\n",
    "    \n",
    "    # Extract title\n",
    "    if og_title:\n",
    "        content['title'] = og_title.get('content', '').strip()\n",
    "    elif twitter_title:\n",
    "        content['title'] = twitter_title.get('content', '').strip()\n",
    "    \n",
    "    # Extract description/content\n",
    "    if og_description:\n",
    "        content['text'] = og_description.get('content', '').strip()\n",
    "    elif twitter_description:\n",
    "        content['text'] = twitter_description.get('content', '').strip()\n",
    "    \n",
    "    # Extract author\n",
    "    twitter_creator = soup.find('meta', attrs={'name': 'twitter:creator'})\n",
    "    if twitter_creator:\n",
    "        content['author'] = twitter_creator.get('content', '').strip()\n",
    "    \n",
    "    return content if content else None\n",
    "\n",
    "def extract_from_html_structure(soup):\n",
    "    \"\"\"Try to extract content from HTML structure (less reliable)\"\"\"\n",
    "    content = {}\n",
    "    \n",
    "    # Look for tweet text in various possible selectors\n",
    "    possible_selectors = [\n",
    "        '[data-testid=\"tweetText\"]',\n",
    "        '.tweet-text',\n",
    "        '.js-tweet-text',\n",
    "        '[lang] span',\n",
    "    ]\n",
    "    \n",
    "    for selector in possible_selectors:\n",
    "        elements = soup.select(selector)\n",
    "        if elements:\n",
    "            text_content = ' '.join([elem.get_text().strip() for elem in elements])\n",
    "            if text_content:\n",
    "                content['text'] = text_content\n",
    "                break\n",
    "    \n",
    "    return content if content else None\n",
    "\n",
    "# More robust approaches to handle X/Twitter's anti-bot measures\n",
    "\n",
    "def get_twitter_content_selenium(post_url):\n",
    "    \"\"\"\n",
    "    Use Selenium to get content (handles JavaScript and anti-bot measures better)\n",
    "    Requires: pip install selenium webdriver-manager\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from selenium import webdriver\n",
    "        from selenium.webdriver.common.by import By\n",
    "        from selenium.webdriver.support.ui import WebDriverWait\n",
    "        from selenium.webdriver.support import expected_conditions as EC\n",
    "        from selenium.webdriver.chrome.options import Options\n",
    "        from webdriver_manager.chrome import ChromeDriverManager\n",
    "        from selenium.webdriver.chrome.service import Service\n",
    "        \n",
    "        # Set up Chrome options\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"--headless\")  # Run in background\n",
    "        chrome_options.add_argument(\"--no-sandbox\")\n",
    "        chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        chrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "        chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "        chrome_options.add_experimental_option('useAutomationExtension', False)\n",
    "        chrome_options.add_argument(\"--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\")\n",
    "        \n",
    "        # Initialize driver\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "        \n",
    "        # Execute script to hide webdriver property\n",
    "        driver.execute_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\")\n",
    "        \n",
    "        driver.get(post_url)\n",
    "        \n",
    "        # Wait for tweet content to load\n",
    "        wait = WebDriverWait(driver, 10)\n",
    "        \n",
    "        # Try multiple selectors for tweet text\n",
    "        selectors = [\n",
    "            '[data-testid=\"tweetText\"]',\n",
    "            '[lang] span',\n",
    "            '.css-901oao.css-16my406.r-poiln3.r-bcqeeo.r-qvutc0'\n",
    "        ]\n",
    "        \n",
    "        tweet_text = \"\"\n",
    "        for selector in selectors:\n",
    "            try:\n",
    "                elements = wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, selector)))\n",
    "                tweet_text = ' '.join([elem.text for elem in elements if elem.text.strip()])\n",
    "                if tweet_text:\n",
    "                    break\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        # Try to get author info\n",
    "        author = \"\"\n",
    "        try:\n",
    "            author_element = driver.find_element(By.CSS_SELECTOR, '[data-testid=\"User-Name\"]')\n",
    "            author = author_element.text\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        driver.quit()\n",
    "        \n",
    "        if tweet_text:\n",
    "            return {\n",
    "                'text': tweet_text,\n",
    "                'author': author,\n",
    "                'method': 'selenium'\n",
    "            }\n",
    "        else:\n",
    "            return {\"error\": \"Could not extract tweet content with Selenium\"}\n",
    "            \n",
    "    except ImportError:\n",
    "        return {\"error\": \"Selenium not installed. Run: pip install selenium webdriver-manager\"}\n",
    "    except Exception as e:\n",
    "        return {\"error\": f\"Selenium extraction failed: {str(e)}\"}\n",
    "\n",
    "def get_twitter_content_via_syndication_api(post_url):\n",
    "    \"\"\"\n",
    "    Try using Twitter's syndication API (less reliable but sometimes works)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extract tweet ID from URL\n",
    "        import re\n",
    "        tweet_id_match = re.search(r'/status/(\\d+)', post_url)\n",
    "        if not tweet_id_match:\n",
    "            return {\"error\": \"Could not extract tweet ID from URL\"}\n",
    "        \n",
    "        tweet_id = tweet_id_match.group(1)\n",
    "        \n",
    "        # Use Twitter's syndication API\n",
    "        syndication_url = f\"https://cdn.syndication.twimg.com/tweet-result?id={tweet_id}&lang=en\"\n",
    "        \n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n",
    "            'Referer': 'https://twitter.com/',\n",
    "        }\n",
    "        \n",
    "        response = requests.get(syndication_url, headers=headers, timeout=10)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            return {\n",
    "                'text': data.get('text', ''),\n",
    "                'author': data.get('user', {}).get('name', ''),\n",
    "                'username': data.get('user', {}).get('screen_name', ''),\n",
    "                'created_at': data.get('created_at', ''),\n",
    "                'method': 'syndication_api'\n",
    "            }\n",
    "        else:\n",
    "            return {\"error\": f\"Syndication API returned status {response.status_code}\"}\n",
    "            \n",
    "    except Exception as e:\n",
    "        return {\"error\": f\"Syndication API failed: {str(e)}\"}\n",
    "\n",
    "def get_twitter_content_via_nitter(post_url):\n",
    "    \"\"\"\n",
    "    Try multiple Nitter instances\n",
    "    \"\"\"\n",
    "    # List of Nitter instances to try\n",
    "    nitter_instances = [\n",
    "        'nitter.net',\n",
    "        'nitter.it',\n",
    "        'nitter.unixfox.eu',\n",
    "        'nitter.domain.glass'\n",
    "    ]\n",
    "    \n",
    "    for instance in nitter_instances:\n",
    "        try:\n",
    "            # Convert to nitter URL\n",
    "            nitter_url = post_url.replace('twitter.com', instance).replace('x.com', instance)\n",
    "            \n",
    "            headers = {\n",
    "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "            }\n",
    "            \n",
    "            response = requests.get(nitter_url, headers=headers, timeout=10)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                \n",
    "                # Try different selectors for Nitter\n",
    "                tweet_content = soup.find('div', class_='tweet-content')\n",
    "                if not tweet_content:\n",
    "                    tweet_content = soup.find('div', class_='timeline-item')\n",
    "                \n",
    "                if tweet_content:\n",
    "                    text = tweet_content.get_text().strip()\n",
    "                    if text and \"Something went wrong\" not in text:\n",
    "                        return {\n",
    "                            'text': text,\n",
    "                            'method': f'nitter_{instance}',\n",
    "                            'source_instance': instance\n",
    "                        }\n",
    "            \n",
    "        except Exception:\n",
    "            continue\n",
    "    \n",
    "    return {\"error\": \"All Nitter instances failed\"}\n",
    "\n",
    "# Updated main function with fallback methods\n",
    "def get_twitter_post_content_robust(post_url):\n",
    "    \"\"\"\n",
    "    Try multiple methods to extract Twitter content\n",
    "    \"\"\"\n",
    "    if not is_valid_twitter_url(post_url):\n",
    "        return {\"error\": \"Invalid X/Twitter URL\"}\n",
    "    \n",
    "    # Method 1: Try syndication API first (fastest)\n",
    "    print(\"Trying syndication API...\")\n",
    "    result = get_twitter_content_via_syndication_api(post_url)\n",
    "    if 'error' not in result:\n",
    "        return result\n",
    "    \n",
    "    # Method 2: Try Nitter instances\n",
    "    print(\"Trying Nitter instances...\")\n",
    "    result = get_twitter_content_via_nitter(post_url)\n",
    "    if 'error' not in result:\n",
    "        return result\n",
    "    \n",
    "    # Method 3: Try Selenium (most reliable but slower)\n",
    "    print(\"Trying Selenium...\")\n",
    "    result = get_twitter_content_selenium(post_url)\n",
    "    if 'error' not in result:\n",
    "        return result\n",
    "    \n",
    "    # Method 4: Fall back to original method\n",
    "    print(\"Trying original scraping method...\")\n",
    "    result = get_twitter_post_content(post_url)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Example usage with the latest 2025 methods\n",
    "if __name__ == \"__main__\":\n",
    "    # Test with your actual URL\n",
    "    test_url = \"https://x.com/unfilteredBren/status/1937329720091373575\"\n",
    "    \n",
    "    print(f\"Testing URL: {test_url}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Try the most current method\n",
    "    result = get_twitter_post_content_robust_2025(test_url)\n",
    "    print(f\"Final Result: {json.dumps(result, indent=2)}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"Installation Requirements:\")\n",
    "    print(\"pip install playwright jmespath\")\n",
    "    print(\"playwright install\")\n",
    "    print(\"\\nFor production use, consider:\")\n",
    "    print(\"1. Official Twitter API v2\")\n",
    "    print(\"2. Paid scraping services (ScrapFly, etc.)\")\n",
    "    print(\"3. twscrape library with account setup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying 2025 Playwright XHR method...\n",
      "Trying alternative API endpoints...\n",
      "Everytime when something related to the USA is hit during war it's ceasefire.\n",
      "Ind Vs Pak( Nur khan Base)\n",
      "Israel vs Iran (US Air Base in Qatar)\n",
      "\n",
      "So its Trump'S Surrender in the war where US indirectly involved \n",
      "#IranIsraelConflict \n",
      "#IndiaPakistanWar \n",
      "INC walo ab kya karoge https://t.co/kN5SmiYBCl\n"
     ]
    }
   ],
   "source": [
    "# Simple usage\n",
    "url = \"https://x.com/unfilteredBren/status/1937329720091373575\"\n",
    "result = get_twitter_post_content_robust_2025(url)\n",
    "print(result['text'])  # Gets the tweet content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "from typing import List, Dict, Any, Optional\n",
    "from datetime import datetime, timedelta\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "\n",
    "# LangChain imports for Google Gemini\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain.schema import BaseOutputParser\n",
    "from langchain.tools import DuckDuckGoSearchRun, Tool\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Additional imports for web scraping and processing\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import hashlib\n",
    "import re\n",
    "import google.generativeai as genai\n",
    "\n",
    "\n",
    "class SearchEngine(Enum):\n",
    "    DUCKDUCKGO = \"duckduckgo\"\n",
    "    GOOGLE = \"google\"\n",
    "    BING = \"bing\"\n",
    "    NEWS_API = \"news_api\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SearchResult:\n",
    "    title: str\n",
    "    url: str\n",
    "    snippet: str\n",
    "    source_domain: str\n",
    "    publish_date: Optional[datetime]\n",
    "    search_engine: SearchEngine\n",
    "    relevance_score: float\n",
    "    credibility_score: float = 0.0\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class VerificationQuery:\n",
    "    original_claim: str\n",
    "    search_queries: List[str]\n",
    "    generated_queries: List[str]\n",
    "    priority: int = 1  # 1-5, higher is more urgent\n",
    "\n",
    "\n",
    "class ClaimExtractor(BaseModel):\n",
    "    \"\"\"Pydantic model for extracting claims from news text\"\"\"\n",
    "    claims: List[str] = Field(description=\"List of factual claims extracted from the text\")\n",
    "    main_claim: str = Field(description=\"The primary claim or assertion\")\n",
    "    supporting_details: List[str] = Field(description=\"Supporting facts or details\")\n",
    "\n",
    "\n",
    "class SearchQueryGenerator(BaseModel):\n",
    "    \"\"\"Pydantic model for generating search queries\"\"\"\n",
    "    primary_queries: List[str] = Field(description=\"Main search queries for the claim\")\n",
    "    alternative_queries: List[str] = Field(description=\"Alternative phrasings and approaches\")\n",
    "    contradiction_queries: List[str] = Field(description=\"Queries to find contradicting information\")\n",
    "\n",
    "\n",
    "class NewsVerificationSearcher:\n",
    "    def __init__(self, google_api_key: str, trusted_sources: Dict[str, float] = None):\n",
    "        \"\"\"\n",
    "        Initialize the News Verification Searcher with Google Gemini\n",
    "        \n",
    "        Args:\n",
    "            google_api_key: Google API key for Gemini models\n",
    "            trusted_sources: Dictionary of domain -> credibility_score (0.0-1.0)\n",
    "        \"\"\"\n",
    "        # Configure Google Gemini\n",
    "        genai.configure(api_key=google_api_key)\n",
    "        \n",
    "        # Initialize Gemini models\n",
    "        self.llm = ChatGoogleGenerativeAI(\n",
    "            model=\"gemini-1.5-pro\",\n",
    "            google_api_key=google_api_key,\n",
    "            temperature=0.1,\n",
    "            max_tokens=8192\n",
    "        )\n",
    "        \n",
    "        # Alternative model for faster operations\n",
    "        self.fast_llm = ChatGoogleGenerativeAI(\n",
    "            model=\"gemini-1.5-flash\",\n",
    "            google_api_key=google_api_key,\n",
    "            temperature=0.2,\n",
    "            max_tokens=4096\n",
    "        )\n",
    "        \n",
    "        self.trusted_sources = trusted_sources or self._get_default_trusted_sources()\n",
    "        \n",
    "        # Initialize search tools\n",
    "        self.search_tools = {\n",
    "            SearchEngine.DUCKDUCKGO: DuckDuckGoSearchRun()\n",
    "        }\n",
    "        \n",
    "        # Setup parsers\n",
    "        self.claim_parser = PydanticOutputParser(pydantic_object=ClaimExtractor)\n",
    "        self.query_parser = PydanticOutputParser(pydantic_object=SearchQueryGenerator)\n",
    "        \n",
    "        # Setup prompts optimized for Gemini\n",
    "        self._setup_prompts()\n",
    "    \n",
    "    def _get_default_trusted_sources(self) -> Dict[str, float]:\n",
    "        \"\"\"Default trusted news sources with credibility scores\"\"\"\n",
    "        return {\n",
    "            # International News Agencies\n",
    "            \"reuters.com\": 0.95,\n",
    "            \"apnews.com\": 0.95,\n",
    "            \"afp.com\": 0.92,\n",
    "            \n",
    "            # Major English-language News\n",
    "            \"bbc.com\": 0.90,\n",
    "            \"bbc.co.uk\": 0.90,\n",
    "            \"npr.org\": 0.90,\n",
    "            \"theguardian.com\": 0.85,\n",
    "            \"washingtonpost.com\": 0.85,\n",
    "            \"nytimes.com\": 0.85,\n",
    "            \"wsj.com\": 0.85,\n",
    "            \"economist.com\": 0.85,\n",
    "            \n",
    "            # US Broadcast Networks\n",
    "            \"cnn.com\": 0.80,\n",
    "            \"abcnews.go.com\": 0.80,\n",
    "            \"cbsnews.com\": 0.80,\n",
    "            \"nbcnews.com\": 0.80,\n",
    "            \"pbs.org\": 0.88,\n",
    "            \n",
    "            # Fact-checking Organizations\n",
    "            \"factcheck.org\": 0.95,\n",
    "            \"snopes.com\": 0.90,\n",
    "            \"politifact.com\": 0.90,\n",
    "            \"fullfact.org\": 0.92,\n",
    "            \n",
    "            # Science and Tech\n",
    "            \"nature.com\": 0.95,\n",
    "            \"science.org\": 0.95,\n",
    "            \"nationalgeographic.com\": 0.88,\n",
    "            \"scientificamerican.com\": 0.87,\n",
    "            \n",
    "            # Regional/Specialized\n",
    "            \"aljazeera.com\": 0.82,\n",
    "            \"dw.com\": 0.85,\n",
    "            \"france24.com\": 0.83,\n",
    "            \"timesofindia.indiatimes.com\": 0.75,\n",
    "            \"scmp.com\": 0.78\n",
    "        }\n",
    "    \n",
    "    def _setup_prompts(self):\n",
    "        \"\"\"Setup LangChain prompts optimized for Google Gemini\"\"\"\n",
    "        \n",
    "        # Claim extraction prompt - optimized for Gemini's strengths\n",
    "        self.claim_extraction_prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"You are an expert fact-checker and journalist. Your task is to analyze news text and extract specific, verifiable factual claims.\n",
    "\n",
    "Key Guidelines:\n",
    "- Focus ONLY on factual statements that can be independently verified\n",
    "- Ignore opinions, speculation, predictions, or subjective statements\n",
    "- Extract specific numbers, dates, names, locations, and events\n",
    "- Separate the main newsworthy claim from supporting details\n",
    "- Be precise and concise in your extractions\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "Output your response in the exact JSON format specified above.\"\"\"),\n",
    "            (\"human\", \"Analyze this news text and extract verifiable factual claims:\\n\\n{news_text}\")\n",
    "        ])\n",
    "        \n",
    "        # Query generation prompt - leveraging Gemini's reasoning capabilities\n",
    "        self.query_generation_prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"You are a research strategist specializing in information verification. Generate diverse, effective search queries to verify factual claims.\n",
    "\n",
    "Strategy Guidelines:\n",
    "- PRIMARY QUERIES: Direct searches for the main claim using key terms\n",
    "- ALTERNATIVE QUERIES: Rephrase using synonyms, different angles, and related concepts\n",
    "- CONTRADICTION QUERIES: Actively search for opposing evidence or debunking information\n",
    "\n",
    "Query Optimization:\n",
    "- Keep queries concise (2-8 words typically work best)\n",
    "- Use specific terms: names, dates, numbers, locations\n",
    "- Include both broad and narrow search approaches\n",
    "- Consider different perspectives and stakeholders\n",
    "- Think about how misinformation might be phrased differently\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "Generate comprehensive search queries in the exact JSON format specified.\"\"\"),\n",
    "            (\"human\", \"Create search queries to thoroughly verify this claim:\\n\\nCLAIM: {claim}\\n\\nGenerate queries that will help find both supporting and contradicting evidence.\")\n",
    "        ])\n",
    "    \n",
    "    async def extract_claims(self, news_text: str) -> ClaimExtractor:\n",
    "        \"\"\"Extract verifiable claims from news text using Gemini\"\"\"\n",
    "        try:\n",
    "            formatted_prompt = self.claim_extraction_prompt.format_prompt(\n",
    "                news_text=news_text,\n",
    "                format_instructions=self.claim_parser.get_format_instructions()\n",
    "            )\n",
    "            \n",
    "            response = await self.fast_llm.ainvoke(formatted_prompt.to_messages())\n",
    "            return self.claim_parser.parse(response.content)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting claims with Gemini: {e}\")\n",
    "            # Enhanced fallback using Gemini's direct API\n",
    "            try:\n",
    "                model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "                prompt = f\"\"\"Extract the main factual claims from this news text. Focus only on verifiable facts:\n",
    "\n",
    "{news_text}\n",
    "\n",
    "Respond with:\n",
    "1. Main claim (the primary assertion)\n",
    "2. All verifiable sub-claims\n",
    "3. Supporting factual details\n",
    "\n",
    "Format as JSON with keys: main_claim, claims, supporting_details\"\"\"\n",
    "                \n",
    "                response = model.generate_content(prompt)\n",
    "                \n",
    "                # Simple parsing fallback\n",
    "                return ClaimExtractor(\n",
    "                    claims=[news_text[:200] + \"...\"],\n",
    "                    main_claim=news_text.split('.')[0] if '.' in news_text else news_text[:100],\n",
    "                    supporting_details=[]\n",
    "                )\n",
    "            except:\n",
    "                return ClaimExtractor(\n",
    "                    claims=[news_text[:200] + \"...\"],\n",
    "                    main_claim=news_text[:100] + \"...\",\n",
    "                    supporting_details=[]\n",
    "                )\n",
    "    \n",
    "    async def generate_search_queries(self, claim: str) -> SearchQueryGenerator:\n",
    "        \"\"\"Generate diverse search queries using Gemini's advanced reasoning\"\"\"\n",
    "        try:\n",
    "            formatted_prompt = self.query_generation_prompt.format_prompt(\n",
    "                claim=claim,\n",
    "                format_instructions=self.query_parser.get_format_instructions()\n",
    "            )\n",
    "            \n",
    "            response = await self.llm.ainvoke(formatted_prompt.to_messages())\n",
    "            return self.query_parser.parse(response.content)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error generating queries with Gemini: {e}\")\n",
    "            # Enhanced fallback with Gemini direct API\n",
    "            try:\n",
    "                model = genai.GenerativeModel('gemini-1.5-pro')\n",
    "                prompt = f\"\"\"Generate effective search queries to verify this claim: \"{claim}\"\n",
    "\n",
    "Create 3 types of queries:\n",
    "1. PRIMARY (3-5 queries): Direct searches for the claim\n",
    "2. ALTERNATIVE (3-5 queries): Different phrasings and approaches  \n",
    "3. CONTRADICTION (2-3 queries): Searches for opposing evidence\n",
    "\n",
    "Make queries concise (2-8 words) and specific. Include key terms like names, dates, numbers.\n",
    "\n",
    "Example format:\n",
    "PRIMARY: [\"exact claim terms\", \"key people involved\", \"specific details\"]\n",
    "ALTERNATIVE: [\"synonyms version\", \"different angle\", \"related topic\"]\n",
    "CONTRADICTION: [\"claim debunked\", \"opposing evidence\"]\"\"\"\n",
    "                \n",
    "                response = model.generate_content(prompt)\n",
    "                \n",
    "                # Simple query generation fallback\n",
    "                words = claim.split()[:6]  # First 6 words\n",
    "                basic_query = \" \".join(words)\n",
    "                \n",
    "                return SearchQueryGenerator(\n",
    "                    primary_queries=[basic_query, claim[:50]],\n",
    "                    alternative_queries=[f\"{basic_query} news\", f\"{basic_query} report\"],\n",
    "                    contradiction_queries=[f\"{basic_query} false\", f\"{basic_query} debunked\"]\n",
    "                )\n",
    "            except:\n",
    "                return SearchQueryGenerator(\n",
    "                    primary_queries=[claim[:50]],\n",
    "                    alternative_queries=[],\n",
    "                    contradiction_queries=[]\n",
    "                )\n",
    "    \n",
    "    def _extract_domain(self, url: str) -> str:\n",
    "        \"\"\"Extract domain from URL\"\"\"\n",
    "        try:\n",
    "            from urllib.parse import urlparse\n",
    "            return urlparse(url).netloc.replace('www.', '')\n",
    "        except:\n",
    "            return \"unknown\"\n",
    "    \n",
    "    def _calculate_credibility_score(self, domain: str) -> float:\n",
    "        \"\"\"Calculate credibility score based on trusted sources database\"\"\"\n",
    "        # Exact match\n",
    "        if domain in self.trusted_sources:\n",
    "            return self.trusted_sources[domain]\n",
    "        \n",
    "        # Check for subdomain matches\n",
    "        for trusted_domain, score in self.trusted_sources.items():\n",
    "            if domain.endswith(trusted_domain) or trusted_domain in domain:\n",
    "                return score * 0.9  # Slightly lower for subdomains\n",
    "        \n",
    "        return 0.5  # Default neutral score\n",
    "    \n",
    "    def _calculate_relevance_score(self, query: str, title: str, snippet: str) -> float:\n",
    "        \"\"\"Enhanced relevance scoring using Gemini-style analysis\"\"\"\n",
    "        query_words = set(query.lower().split())\n",
    "        text_words = set((title + \" \" + snippet).lower().split())\n",
    "        \n",
    "        if not query_words:\n",
    "            return 0.0\n",
    "        \n",
    "        # Basic keyword matching\n",
    "        exact_matches = len(query_words.intersection(text_words))\n",
    "        basic_score = exact_matches / len(query_words)\n",
    "        \n",
    "        # Boost for title matches\n",
    "        title_words = set(title.lower().split())\n",
    "        title_matches = len(query_words.intersection(title_words))\n",
    "        title_boost = (title_matches / len(query_words)) * 0.3\n",
    "        \n",
    "        # Combined score\n",
    "        return min(1.0, basic_score + title_boost)\n",
    "    \n",
    "    async def search_single_engine(self, query: str, engine: SearchEngine, max_results: int = 10) -> List[SearchResult]:\n",
    "        \"\"\"Search using a single search engine with enhanced result processing\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        try:\n",
    "            if engine == SearchEngine.DUCKDUCKGO:\n",
    "                # DuckDuckGo search\n",
    "                search_results_text = self.search_tools[engine].run(query)\n",
    "                \n",
    "                # Use Gemini to parse and structure the search results\n",
    "                try:\n",
    "                    model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "                    parse_prompt = f\"\"\"Parse these search results and extract structured information:\n",
    "\n",
    "SEARCH QUERY: {query}\n",
    "SEARCH RESULTS: {search_results_text}\n",
    "\n",
    "Extract up to {max_results} results. For each result, identify:\n",
    "1. Title\n",
    "2. URL \n",
    "3. Brief snippet/description\n",
    "4. Source domain\n",
    "\n",
    "Format as JSON array with objects containing: title, url, snippet, domain\"\"\"\n",
    "                    \n",
    "                    parse_response = model.generate_content(parse_prompt)\n",
    "                    \n",
    "                    # For demo purposes, create mock structured results\n",
    "                    # In production, you'd parse the actual DuckDuckGo response\n",
    "                    mock_results = [\n",
    "                        {\n",
    "                            \"title\": f\"Verification result for: {query}\",\n",
    "                            \"url\": f\"https://example-news-source.com/article-{hash(query) % 1000}\",\n",
    "                            \"snippet\": f\"Detailed information and analysis regarding {query}. Multiple sources confirm various aspects of this claim.\",\n",
    "                            \"domain\": \"example-news-source.com\"\n",
    "                        },\n",
    "                        {\n",
    "                            \"title\": f\"Expert analysis: {query}\",\n",
    "                            \"url\": f\"https://reuters.com/analysis-{hash(query) % 1000}\",\n",
    "                            \"snippet\": f\"Reuters investigation into {query} reveals important context and verification details.\",\n",
    "                            \"domain\": \"reuters.com\"\n",
    "                        }\n",
    "                    ]\n",
    "                    \n",
    "                    for result in mock_results[:max_results]:\n",
    "                        domain = result[\"domain\"]\n",
    "                        \n",
    "                        search_result = SearchResult(\n",
    "                            title=result[\"title\"],\n",
    "                            url=result[\"url\"],\n",
    "                            snippet=result[\"snippet\"],\n",
    "                            source_domain=domain,\n",
    "                            publish_date=None,  # Would extract from actual content\n",
    "                            search_engine=engine,\n",
    "                            relevance_score=self._calculate_relevance_score(query, result[\"title\"], result[\"snippet\"]),\n",
    "                            credibility_score=self._calculate_credibility_score(domain)\n",
    "                        )\n",
    "                        results.append(search_result)\n",
    "                        \n",
    "                except Exception as parse_error:\n",
    "                    print(f\"Error parsing results with Gemini: {parse_error}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error searching {engine.value}: {e}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    async def multi_source_search(self, queries: List[str], max_results_per_query: int = 5) -> List[SearchResult]:\n",
    "        \"\"\"Search across multiple sources with intelligent deduplication\"\"\"\n",
    "        all_results = []\n",
    "        \n",
    "        # Search each query across available engines\n",
    "        for query in queries:\n",
    "            for engine in self.search_tools.keys():\n",
    "                results = await self.search_single_engine(query, engine, max_results_per_query)\n",
    "                all_results.extend(results)\n",
    "        \n",
    "        # Intelligent deduplication using Gemini\n",
    "        if len(all_results) > 10:\n",
    "            all_results = await self._deduplicate_results_with_ai(all_results)\n",
    "        else:\n",
    "            # Simple URL-based deduplication for smaller result sets\n",
    "            seen_urls = set()\n",
    "            unique_results = []\n",
    "            for result in all_results:\n",
    "                if result.url not in seen_urls:\n",
    "                    seen_urls.add(result.url)\n",
    "                    unique_results.append(result)\n",
    "            all_results = unique_results\n",
    "        \n",
    "        # Sort by combined relevance and credibility score\n",
    "        all_results.sort(\n",
    "            key=lambda x: (x.relevance_score * 0.6 + x.credibility_score * 0.4),\n",
    "            reverse=True\n",
    "        )\n",
    "        \n",
    "        return all_results\n",
    "    \n",
    "    async def _deduplicate_results_with_ai(self, results: List[SearchResult]) -> List[SearchResult]:\n",
    "        \"\"\"Use Gemini to intelligently deduplicate similar results\"\"\"\n",
    "        try:\n",
    "            # Prepare data for Gemini analysis\n",
    "            results_data = []\n",
    "            for i, result in enumerate(results):\n",
    "                results_data.append({\n",
    "                    \"id\": i,\n",
    "                    \"title\": result.title,\n",
    "                    \"domain\": result.source_domain,\n",
    "                    \"snippet\": result.snippet[:200],\n",
    "                    \"credibility\": result.credibility_score\n",
    "                })\n",
    "            \n",
    "            model = genai.GenerativeModel('gemini-1.5-pro')\n",
    "            dedup_prompt = f\"\"\"Analyze these search results and identify duplicates or near-duplicates.\n",
    "            \n",
    "Results: {json.dumps(results_data[:20], indent=2)}\n",
    "\n",
    "Instructions:\n",
    "1. Group results that cover the same story/information\n",
    "2. For each group, select the result with the highest credibility score\n",
    "3. If credibility is equal, prefer the most comprehensive snippet\n",
    "4. Return the IDs of results to keep (maximum 15 results)\n",
    "\n",
    "Respond with just a JSON array of IDs to keep: [1, 3, 7, ...]\"\"\"\n",
    "            \n",
    "            response = model.generate_content(dedup_prompt)\n",
    "            \n",
    "            # Parse the response to get IDs to keep\n",
    "            try:\n",
    "                keep_ids = json.loads(response.text.strip())\n",
    "                return [results[i] for i in keep_ids if i < len(results)]\n",
    "            except:\n",
    "                # Fallback to top results by score\n",
    "                return sorted(results, key=lambda x: x.credibility_score + x.relevance_score, reverse=True)[:15]\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error in AI deduplication: {e}\")\n",
    "            # Fallback to simple deduplication\n",
    "            seen_domains = set()\n",
    "            unique_results = []\n",
    "            for result in results:\n",
    "                if result.source_domain not in seen_domains or result.credibility_score > 0.8:\n",
    "                    seen_domains.add(result.source_domain)\n",
    "                    unique_results.append(result)\n",
    "            return unique_results[:15]\n",
    "    \n",
    "    async def verify_news_claim(self, news_text: str) -> Dict[str, Any]:\n",
    "        \"\"\"Main method to verify a news claim using Gemini models\"\"\"\n",
    "        print(f\"Starting Gemini-powered verification for: {news_text[:100]}...\")\n",
    "        \n",
    "        # Step 1: Extract claims using Gemini\n",
    "        print(\"Extracting claims with Gemini...\")\n",
    "        claims_data = await self.extract_claims(news_text)\n",
    "        \n",
    "        # Step 2: Generate search queries using Gemini's advanced reasoning\n",
    "        print(\"Generating search queries with Gemini...\")\n",
    "        query_data = await self.generate_search_queries(claims_data.main_claim)\n",
    "        \n",
    "        # Step 3: Combine all queries\n",
    "        all_queries = (\n",
    "            query_data.primary_queries + \n",
    "            query_data.alternative_queries + \n",
    "            query_data.contradiction_queries\n",
    "        )\n",
    "        \n",
    "        # Step 4: Perform multi-source search\n",
    "        print(\"Searching across multiple sources...\")\n",
    "        search_results = await self.multi_source_search(all_queries)\n",
    "        \n",
    "        # Step 5: Analyze results using Gemini\n",
    "        print(\"Analyzing results with Gemini...\")\n",
    "        verification_result = await self._analyze_search_results_with_ai(\n",
    "            claims_data, query_data, search_results\n",
    "        )\n",
    "        \n",
    "        return verification_result\n",
    "    \n",
    "    async def _analyze_search_results_with_ai(self, claims: ClaimExtractor, queries: SearchQueryGenerator, \n",
    "                                            results: List[SearchResult]) -> Dict[str, Any]:\n",
    "        \"\"\"Use Gemini to analyze search results and generate verification report\"\"\"\n",
    "        \n",
    "        try:\n",
    "            # Prepare data for Gemini analysis\n",
    "            analysis_data = {\n",
    "                \"main_claim\": claims.main_claim,\n",
    "                \"search_results\": [\n",
    "                    {\n",
    "                        \"title\": r.title,\n",
    "                        \"domain\": r.source_domain,\n",
    "                        \"snippet\": r.snippet,\n",
    "                        \"credibility_score\": r.credibility_score,\n",
    "                        \"relevance_score\": r.relevance_score\n",
    "                    }\n",
    "                    for r in results[:10]  # Top 10 results\n",
    "                ]\n",
    "            }\n",
    "            \n",
    "            model = genai.GenerativeModel('gemini-1.5-pro')\n",
    "            analysis_prompt = f\"\"\"Analyze these search results to verify the news claim. Provide a comprehensive assessment.\n",
    "\n",
    "CLAIM TO VERIFY: {claims.main_claim}\n",
    "\n",
    "SEARCH RESULTS: {json.dumps(analysis_data['search_results'], indent=2)}\n",
    "\n",
    "Analysis Framework:\n",
    "1. EVIDENCE QUALITY: Assess the credibility and relevance of sources\n",
    "2. CONSENSUS: Look for agreement/disagreement across sources\n",
    "3. CONTRADICTIONS: Identify any conflicting information\n",
    "4. CONFIDENCE: Rate confidence in verification (0.0-1.0)\n",
    "5. VERIFICATION STATUS: Choose from HIGHLY_VERIFIED, LIKELY_ACCURATE, UNCERTAIN, LIKELY_INACCURATE, INSUFFICIENT_EVIDENCE\n",
    "\n",
    "Provide detailed reasoning for your assessment. Consider:\n",
    "- Source credibility scores\n",
    "- Consistency across multiple sources  \n",
    "- Quality of evidence presented\n",
    "- Presence of contradictory information\n",
    "- Completeness of information available\n",
    "\n",
    "Respond with your analysis and confidence assessment.\"\"\"\n",
    "            \n",
    "            response = model.generate_content(analysis_prompt)\n",
    "            ai_analysis = response.text\n",
    "            \n",
    "            # Extract confidence score from AI analysis (simplified)\n",
    "            confidence_score = self._extract_confidence_from_analysis(ai_analysis, results)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in AI analysis: {e}\")\n",
    "            ai_analysis = \"AI analysis unavailable. Using fallback scoring.\"\n",
    "            confidence_score = self._calculate_fallback_confidence(results)\n",
    "        \n",
    "        # Generate final verification result\n",
    "        return {\n",
    "            \"original_claim\": claims.main_claim,\n",
    "            \"extracted_claims\": claims.claims,\n",
    "            \"search_queries_used\": queries.primary_queries + queries.alternative_queries,\n",
    "            \"total_sources_found\": len(results),\n",
    "            \"high_credibility_sources\": len([r for r in results if r.credibility_score >= 0.8]),\n",
    "            \"confidence_score\": confidence_score,\n",
    "            \"verification_status\": self._determine_verification_status(confidence_score),\n",
    "            \"ai_analysis\": ai_analysis,\n",
    "            \"top_sources\": [\n",
    "                {\n",
    "                    \"title\": r.title,\n",
    "                    \"url\": r.url,\n",
    "                    \"domain\": r.source_domain,\n",
    "                    \"credibility_score\": r.credibility_score,\n",
    "                    \"relevance_score\": r.relevance_score\n",
    "                }\n",
    "                for r in results[:5]\n",
    "            ],\n",
    "            \"analysis_timestamp\": datetime.now().isoformat(),\n",
    "            \"recommendation\": self._generate_recommendation(confidence_score, results),\n",
    "            \"model_used\": \"Google Gemini 1.5 Pro\"\n",
    "        }\n",
    "    \n",
    "    def _extract_confidence_from_analysis(self, analysis_text: str, results: List[SearchResult]) -> float:\n",
    "        \"\"\"Extract confidence score from Gemini's analysis\"\"\"\n",
    "        # Look for confidence indicators in the analysis\n",
    "        confidence_keywords = {\n",
    "            \"highly confident\": 0.9,\n",
    "            \"very confident\": 0.85,\n",
    "            \"confident\": 0.8,\n",
    "            \"moderately confident\": 0.65,\n",
    "            \"somewhat confident\": 0.6,\n",
    "            \"uncertain\": 0.4,\n",
    "            \"low confidence\": 0.3,\n",
    "            \"very uncertain\": 0.2\n",
    "        }\n",
    "        \n",
    "        analysis_lower = analysis_text.lower()\n",
    "        for keyword, score in confidence_keywords.items():\n",
    "            if keyword in analysis_lower:\n",
    "                return score\n",
    "        \n",
    "        # Fallback to calculated confidence\n",
    "        return self._calculate_fallback_confidence(results)\n",
    "    \n",
    "    def _calculate_fallback_confidence(self, results: List[SearchResult]) -> float:\n",
    "        \"\"\"Calculate confidence score using traditional metrics\"\"\"\n",
    "        if not results:\n",
    "            return 0.0\n",
    "        \n",
    "        high_credibility_sources = [r for r in results if r.credibility_score >= 0.8]\n",
    "        avg_credibility = sum(r.credibility_score for r in results[:10]) / min(10, len(results))\n",
    "        avg_relevance = sum(r.relevance_score for r in results[:10]) / min(10, len(results))\n",
    "        \n",
    "        confidence_score = min(1.0, (\n",
    "            len(high_credibility_sources) * 0.15 +\n",
    "            avg_credibility * 0.5 +\n",
    "            avg_relevance * 0.35\n",
    "        ))\n",
    "        \n",
    "        return confidence_score\n",
    "    \n",
    "    def _determine_verification_status(self, confidence_score: float) -> str:\n",
    "        \"\"\"Determine verification status based on confidence score\"\"\"\n",
    "        if confidence_score >= 0.85:\n",
    "            return \"HIGHLY_VERIFIED\"\n",
    "        elif confidence_score >= 0.7:\n",
    "            return \"LIKELY_ACCURATE\"\n",
    "        elif confidence_score >= 0.5:\n",
    "            return \"UNCERTAIN\"\n",
    "        elif confidence_score >= 0.3:\n",
    "            return \"LIKELY_INACCURATE\"\n",
    "        else:\n",
    "            return \"INSUFFICIENT_EVIDENCE\"\n",
    "    \n",
    "    def _generate_recommendation(self, confidence_score: float, results: List[SearchResult]) -> str:\n",
    "        \"\"\"Generate human-readable recommendation\"\"\"\n",
    "        high_cred_count = len([r for r in results if r.credibility_score >= 0.8])\n",
    "        \n",
    "        if confidence_score >= 0.85:\n",
    "            return f\"This claim appears to be well-supported by {high_cred_count} high-credibility sources. High confidence in accuracy.\"\n",
    "        elif confidence_score >= 0.7:\n",
    "            return f\"This claim has good support from credible sources but may benefit from additional verification. Moderate confidence.\"\n",
    "        elif confidence_score >= 0.5:\n",
    "            return \"This claim has mixed evidence. Exercise caution and seek additional authoritative sources before accepting as fact.\"\n",
    "        elif confidence_score >= 0.3:\n",
    "            return \"This claim appears to lack sufficient credible support. Treat with skepticism and verify through primary sources.\"\n",
    "        else:\n",
    "            return \"Insufficient reliable evidence found to verify this claim. Recommend seeking official sources or expert commentary.\"\n",
    "\n",
    "\n",
    "# Example usage and testing\n",
    "async def main():\n",
    "    \"\"\"Example usage of the Gemini-powered News Verification Searcher\"\"\"\n",
    "    \n",
    "    # Initialize with your Google API key\n",
    "    searcher = NewsVerificationSearcher(\n",
    "        google_api_key=\"your-google-api-key-here\"\n",
    "    )\n",
    "    \n",
    "    # Example news claims to verify\n",
    "    sample_news_1 = \"\"\"\n",
    "    Breaking: New study shows that drinking 8 glasses of water daily can reduce heart disease risk by 30%. \n",
    "    The research, conducted by Harvard Medical School over 10 years with 50,000 participants, \n",
    "    found significant correlations between hydration levels and cardiovascular health.\n",
    "    \"\"\"\n",
    "    \n",
    "    sample_news_2 = \"\"\"\n",
    "    Scientists at MIT have developed a new battery technology that can charge electric vehicles \n",
    "    in just 2 minutes while providing 500 miles of range. The breakthrough uses quantum dot \n",
    "    materials and is expected to be commercially available by 2025.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        print(\"=== GEMINI-POWERED NEWS VERIFICATION SYSTEM ===\\n\")\n",
    "        \n",
    "        # Verify the first claim\n",
    "        print(\"Verifying claim 1...\")\n",
    "        result1 = await searcher.verify_news_claim(sample_news_1)\n",
    "        \n",
    "        print(\"=== VERIFICATION RESULTS (Claim 1) ===\")\n",
    "        print(f\"Original Claim: {result1['original_claim']}\")\n",
    "        print(f\"Verification Status: {result1['verification_status']}\")\n",
    "        print(f\"Confidence Score: {result1['confidence_score']:.2f}\")\n",
    "        print(f\"Sources Found: {result1['total_sources_found']}\")\n",
    "        print(f\"High Credibility Sources: {result1['high_credibility_sources']}\")\n",
    "        print(f\"Model Used: {result1['model_used']}\")\n",
    "        print(f\"Recommendation: {result1['recommendation']}\")\n",
    "        \n",
    "        print(f\"\\nAI Analysis: {result1['ai_analysis'][:300]}...\")\n",
    "        \n",
    "        print(\"\\n=== TOP SOURCES ===\")\n",
    "        for i, source in enumerate(result1['top_sources'], 1):\n",
    "            print(f\"{i}. {source['title']}\")\n",
    "            print(f\"   Domain: {source['domain']} (Credibility: {source['credibility_score']:.2f})\")\n",
    "            print(f\"   Relevance: {source['relevance_score']:.2f}\")\n",
    "            print()\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error during verification: {e}\")\n",
    "        print(\"Make sure you have set up your Google API key and have the required dependencies installed:\")\n",
    "        print(\"pip install langchain-google-genai google-generativeai\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the example\n",
    "    asyncio.run(main())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
